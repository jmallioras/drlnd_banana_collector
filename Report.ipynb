{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "This report concearns the implementation of the deep reinforcement learning course navigation project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Training the agent\n",
    "\n",
    "#### a. DQN architecture and hyperparameters\n",
    "To train the agent, we used deep Q-learning. Therefore, the state-action Q-values are produced by a neural network. The design used is a multilayer perceptron with 2 hidden layers of sizes 128 and 256 respectively while tanh is the activation function used for both layers. For training, learning rate is set to  0.001 and batch size is 128.The hyperparameters of the DQN (layer sizes and activation functions) are defined inside *rl_agent.py*.\n",
    "\n",
    "#### b. Techniques\n",
    "Using experience replay, we save the experiences (tuples containing {state, action, reward} information) into a **replay buffer** (defined in *rl_agent.py*). Once the replay buffer contains more experiences than the **batch size** we set (here it is 128) we can train the neural network. The neural network is not trained after each step the agent takes, but in regular intervals of **5 steps** as defined in the parameter **UPDATE_EVERY** inside the *rl_agent.py*. Using two identical DQNs, the **target network** and the **local network**, we are able to save the old version of the DQN into the target DQN, then train the local DQN and use it to collect more experiences. When the agent takes 5 steps with this new DQN, we compare the Q values produced by the target DQN with those of the local DQN on 128 randomly selected experience samples.\n",
    "\n",
    "#### c. Cost function and training termination criteria\n",
    "With MSE (mean square error) as the cost function, we propagate the error between the previous version and the new version into the local DQN, updating the weights of its neurons, in order to further improve its performance. Training stops either at 1000 episodes or when the average score of the last 100 episodes is >14.0.\n",
    "#### Let's train the model and see how the score improves in time:\n",
    "![training](scores.png)\n",
    "\n",
    "Training terminated in epsiode 650 because **the average score between episodes 550-650 is >14**. Training lasted 21 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Evaluating the trained DQN-based agent.\n",
    "After having trained the DQN agent, we can evaluate its performance. The goal is to achieve an **average score of >13.0 in 100 episodes**. The progress of the score on each episode is shown below:\n",
    "\n",
    "![evaluating](eval_scores.png)\n",
    "\n",
    "The average score here is **14.19!**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3. Ideas for Future Work\n",
    "\n",
    "To improve the agent presented above, we can implement some extentions to the Q-learning algorithm such as\n",
    "\n",
    "- **prioritzied experience replay**\n",
    "- a **double DQN**\n",
    "- a **dueling DQN**\n",
    "\n",
    "In this project, the state is defined by a vector of 37 values which represent location, velocity of the agent and other environment metrics. However, we can perform the same project simply by using the image of the unity environment. The state can be represented by the pixels of each frame of the game. This task is much more complex and it demands the use of a **Convolutional Neural Network** as the DQN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}